{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PDKbNnWRw0uO"
      },
      "outputs": [],
      "source": [
        "#References\n",
        "#https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
        "#https://pasaentuciudad.com.mx/data-to-model-to-api-an-end-to-end-approach/\n",
        "#https://github.com/The-AI-Summer/Deep-Learning-In-Production/tree/master/2.%20Writing%20Deep%20Learning%20code:%20Best%20Practises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8H-02yf-xC7x"
      },
      "source": [
        "# Install and import the necessary dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j7r9V2T_w-Rl"
      },
      "outputs": [],
      "source": [
        "\n",
        "from google.colab import drive\n",
        "\n",
        "#This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Create a symbolic link, in order no to be able to save weights on drive. Otherwise it gives an error. \n",
        "!ln -s /content/drive/My\\ Drive /content/mydrive\n",
        "\n",
        "\n",
        "#Install requirements\n",
        "%cd mydrive/SentimentAnalysis\n",
        "!pip install -r requirements.txt \n",
        "\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import requests, zipfile, io\n",
        "\n",
        "import pandas as pd\n",
        "# import re\n",
        "# import wordninja, contractions, emoji\n",
        "\n",
        "import seaborn as sns #visualization.py\n",
        "import nltk #visualization.py\n",
        "from nltk.corpus import stopwords #visualization.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "sys.path.insert(0,'/content/mydrive/SentimentAnalysis/src/utils')\n",
        "import Config, preprocess\n",
        "\n",
        "sys.path.insert(0,'/content/mydrive/SentimentAnalysis/src/data')\n",
        "import dataloader\n",
        "\n",
        "sys.path.insert(0,'/content/mydrive/SentimentAnalysis/src/models/')\n",
        "import BaseModel,LSTM\n",
        "\n",
        "sys.path.insert(0,'/content/mydrive/SentimentAnalysis/configs')\n",
        "from config_main import CFG\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oDhp8yPlxQTW"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hefMqAF-Phsa"
      },
      "outputs": [],
      "source": [
        "\n",
        "input_path = os.path.join('/content/mydrive/SentimentAnalysis/datasets', CFG['data']['name'])\n",
        "\n",
        "#Download and unzip dataset\n",
        "if not (os.path.isfile(CFG['data']['url'].split('/')[-1])):\n",
        "  dataloader.download_dataset_from_url(CFG['data']['url'])\n",
        "dataloader.unzip_data_to_flder(input_path, CFG['data']['url'].split('/')[-1])\n",
        "\n",
        "#Read unzipped data\n",
        "train_data_df = pd.read_csv(os.path.join(input_path, 'training.1600000.processed.noemoticon.csv'),  encoding='latin-1', usecols=[0,5], names=['sentiment','tweet'])\n",
        "\n",
        "#Print dataset info\n",
        "print(train_data_df.info())  \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMcYrpxB3u_7"
      },
      "source": [
        "# Preprocess data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WwxW9HXv-pkL"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Preprocessing tweets data\n",
        "print(\"Cleaning and parsing the tweets...\\n\")\n",
        "train_data_df = train_data_df.iloc[0:10000] #TO-DO: Remove\n",
        "train_data_df.tweet = train_data_df.tweet.apply(datacleaner.preprocess_tweet)\n",
        "print(\"Finished!\\n\")\n",
        "\n",
        "# Preprocessing labels to have classes 0 and 1\n",
        "train_data_df.sentiment = train_data_df.sentiment.apply(lambda value: 1 if value==4 \n",
        "                                                        else value)\n",
        "\n",
        "print(\"Sentiment values: \")\n",
        "print(train_data_df.sentiment.value_counts())\n",
        "\n",
        "print(train_data_df.head(5)) \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUoDy_SLa3WD"
      },
      "source": [
        "# Analyze data\n",
        "Estimating the vocabulary size and the vector sequence length to be fed to the model every instance is a crucial step for a good model.\n",
        "\n",
        "This is achieved by analyzing the training dataset, by plotting the \n",
        "distribution of tweet lengths across the training data.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LzakgPb2QY-T"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Return the length of a tweet.\n",
        "Input: tweet(str)\n",
        "Output: length(int)\n",
        "\"\"\"\n",
        "def tweet_length(tweet):\n",
        "  return len([token for token in tweet.split()])\n",
        "\n",
        "tweet_lengths = [tweet_length(tweet) for tweet in train_data_df.tweet.tolist()]\n",
        "sns.distplot(tweet_lengths)\n",
        "\n",
        "# Unique words\n",
        "unique_words = set([token for tweet in train_data_df.tweet for token in tweet.split()])\n",
        "print(\"Total Unique Words:\", len(unique_words))\n",
        "\n",
        "# Counting Total Words and Stop Words\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = stopwords.words(\"english\")\n",
        "total_words = [token for tweet in train_data_df.tweet for token in tweet.split()]\n",
        "total_stop_words = [token for tweet in train_data_df.tweet for token in tweet.split() if token in stop_words]\n",
        "print('Total words', len(total_words))\n",
        "print('Total stop words', len(total_stop_words))\n",
        "print('Ratio of total words to total stop words:', len(total_words)/len(total_stop_words))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rspb76kIluXY"
      },
      "source": [
        "# Train model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N0eU2qradaKb"
      },
      "outputs": [],
      "source": [
        "from importlib import reload  # Py3 only; unneeded in py2.\n",
        "foo = reload(LSTM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HG3NE-k2EPrf",
        "outputId": "bbcf982e-0398-4947-ff38-af343cfd33de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1]\n",
            "<keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x7eff3c4d8b10>\n",
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 1)]               0         \n",
            "                                                                 \n",
            " text_vectorization_2 (TextV  (None, 50)               0         \n",
            " ectorization)                                                   \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, 50, 128)           9600128   \n",
            "                                                                 \n",
            " LSTM_1 (LSTM)               (None, 50, 256)           394240    \n",
            "                                                                 \n",
            " LSTM_2 (LSTM)               (None, 256)               525312    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 256)               0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 64)                16448     \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 10,536,193\n",
            "Trainable params: 10,536,193\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Epoch 1/3\n",
            "7/7 [==============================] - 7s 436ms/step - loss: 0.3569 - accuracy: 0.8573 - val_loss: 1.2949e-04 - val_accuracy: 1.0000\n",
            "Epoch 2/3\n",
            "7/7 [==============================] - 2s 273ms/step - loss: 3.4230e-05 - accuracy: 1.0000 - val_loss: 5.1644e-07 - val_accuracy: 1.0000\n",
            "Epoch 3/3\n",
            "7/7 [==============================] - 2s 269ms/step - loss: 2.6898e-07 - accuracy: 1.0000 - val_loss: 4.3918e-08 - val_accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "([0.35688239336013794, 3.422956433496438e-05, 2.689753841877973e-07],\n",
              " [0.00012949264782946557, 5.164381491340464e-07, 4.3917900427459244e-08])"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model = LSTM.LSTM(CFG)\n",
        "model.split_training_data(train_data_df)\n",
        "model.data_vectorization(75000, 50)\n",
        "model.build_model()\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2kUhFeFiG3Mx"
      },
      "source": [
        "# Save/Serialize model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDGS7CzMocnQ"
      },
      "outputs": [],
      "source": [
        "# Saving Model\n",
        "MODEL_DIR = \"/content/mydrive/SentimentAnalysis/models\"\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format=None,\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "\n",
        "# Check the path\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}\n",
        "\n",
        "# Using SavedModelCLI to check if model is persisted properly\n",
        "!saved_model_cli show --dir {export_path} --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kE1NmDUCG2oW"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XamCNmTfEPzh"
      },
      "outputs": [],
      "source": [
        "model.evaluate() #TO-DO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RDpHQnlBjSvC"
      },
      "source": [
        "# Evaluate model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jKgxojXgpahr"
      },
      "outputs": [],
      "source": [
        "# Loading and Evaluation of Model\n",
        "\n",
        "test_data_df = pd.read_csv(os.path.join(input_path, 'testdata.manual.2009.06.14.csv'),  encoding='latin-1', usecols=[0,5], names=['sentiment','tweet'])\n",
        "print(test_data_df.sentiment.value_counts()/test_data_df.shape[0])\n",
        "\n",
        "# Preprocessing tweets data\n",
        "print(\"Cleaning and parsing the tweets...\\n\")\n",
        "test_data_df = test_data_df.iloc[0:1000] #TO-DO: Remove\n",
        "test_data_df.tweet = test_data_df.tweet.apply(preprocess_tweet) #TO-DO\n",
        "print(\"Finished!\\n\")\n",
        "\n",
        "test_data_df = test_data_df[test_data_df.sentiment!=2] #Remove intermediate polarities\n",
        "test_data_df.sentiment.value_counts()/test_data_df.shape[0]\n",
        "\n",
        "test_data_df.sentiment = test_data_df.sentiment.apply(lambda value: 1 if value==4 else value)\n",
        "\n",
        "print(test_data_df.sentiment.value_counts()/test_data_df.shape[0])\n",
        "\n",
        "\n",
        "# Preprocessing labels to have classes 0 and 1\n",
        "test_data_df.sentiment = test_data_df.sentiment.apply(lambda value: 1 if value==4 else value)\n",
        "\n",
        "#Create data pipeline for test\n",
        "test_dataset = create_data_pipeline(test_data_df.tweet, test_data_df.sentiment, batch_size=128, is_training=False)\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model(export_path)\n",
        "model.evaluate(test_dataset)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "SentimentAnalysis.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}