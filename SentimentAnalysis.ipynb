{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SentimentAnalysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "PDKbNnWRw0uO"
      },
      "outputs": [],
      "source": [
        "#References\n",
        "#https://towardsdatascience.com/another-twitter-sentiment-analysis-bb5b01ebad90\n",
        "#https://pasaentuciudad.com.mx/data-to-model-to-api-an-end-to-end-approach/"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and import the necessary dependencies"
      ],
      "metadata": {
        "id": "8H-02yf-xC7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Dependencies\n",
        "!pip install wordninja contractions emoji\n",
        "\n",
        "import os\n",
        "import requests, zipfile, io\n",
        "from google.colab import drive\n",
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "import wordninja, contractions, emoji #preprocessing.py\n",
        "\n",
        "import seaborn as sns #visualization.py\n",
        "import nltk #visualization.py\n",
        "from nltk.corpus import stopwords #visualization.py\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "#This will prompt for authorization.\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#Create a symbolic link, in order no to be able to save weights on drive. Otherwise it gives an error. \n",
        "!ln -s /content/drive/My\\ Drive /content/mydrive\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j7r9V2T_w-Rl",
        "outputId": "77b940a0-fd80-4362-d9d6-27604ea31e99"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wordninja in /usr/local/lib/python3.7/dist-packages (2.0.0)\n",
            "Requirement already satisfied: contractions in /usr/local/lib/python3.7/dist-packages (0.1.68)\n",
            "Requirement already satisfied: emoji in /usr/local/lib/python3.7/dist-packages (1.7.0)\n",
            "Requirement already satisfied: textsearch>=0.0.21 in /usr/local/lib/python3.7/dist-packages (from contractions) (0.0.21)\n",
            "Requirement already satisfied: anyascii in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (0.3.0)\n",
            "Requirement already satisfied: pyahocorasick in /usr/local/lib/python3.7/dist-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "ln: failed to create symbolic link '/content/mydrive/My Drive': Input/output error\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Configuration file\n",
        "\n",
        "# dataset_name = 'Sentiment140'\n",
        "# input_path = os.path.join('/content/mydrive', dataset_name)\n",
        "# dataset_url = 'http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip'\n"
      ],
      "metadata": {
        "id": "w4HV05dYxAmy"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load data"
      ],
      "metadata": {
        "id": "oDhp8yPlxQTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Download and unzip dataset\n",
        "!wget http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
        "!mkdir '/content/mydrive/SentimentAnalysis'\n",
        "!mkdir '/content/mydrive/SentimentAnalysis/Input'\n",
        "!unzip trainingandtestdata.zip -d '/content/mydrive/SentimentAnalysis/Input'\n",
        "\n",
        "input_path = '/content/mydrive/SentimentAnalysis/Input'\n",
        "\n",
        "#Read unzipped data\n",
        "train_data_df = pd.read_csv(os.path.join(input_path, 'training.1600000.processed.noemoticon.csv'),  encoding='latin-1', usecols=[0,5], names=['sentiment','tweet'])\n",
        "\n",
        "#Print dataset info\n",
        "print(train_data_df.info())  \n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hefMqAF-Phsa",
        "outputId": "550f6404-bc68-42b2-f9c7-e0745b213180"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2022-04-03 18:42:17--  https://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip\n",
            "Resolving cs.stanford.edu (cs.stanford.edu)... 171.64.64.64\n",
            "Connecting to cs.stanford.edu (cs.stanford.edu)|171.64.64.64|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 81363704 (78M) [application/zip]\n",
            "Saving to: ‘trainingandtestdata.zip.4’\n",
            "\n",
            "trainingandtestdata 100%[===================>]  77.59M  47.1MB/s    in 1.6s    \n",
            "\n",
            "2022-04-03 18:42:19 (47.1 MB/s) - ‘trainingandtestdata.zip.4’ saved [81363704/81363704]\n",
            "\n",
            "mkdir: cannot create directory ‘/content/mydrive/SentimentAnalysis’: File exists\n",
            "mkdir: cannot create directory ‘/content/mydrive/SentimentAnalysis/Input’: File exists\n",
            "Archive:  trainingandtestdata.zip\n",
            "replace /content/mydrive/SentimentAnalysis/Input/testdata.manual.2009.06.14.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: <class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1600000 entries, 0 to 1599999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count    Dtype \n",
            "---  ------     --------------    ----- \n",
            " 0   sentiment  1600000 non-null  int64 \n",
            " 1   tweet      1600000 non-null  object\n",
            "dtypes: int64(1), object(1)\n",
            "memory usage: 24.4+ MB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocess data"
      ],
      "metadata": {
        "id": "PMcYrpxB3u_7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def strip_emoji(tweet):\n",
        "  new_tweet = re.sub(emoji.get_emoji_regexp(), r\"\", tweet)\n",
        "  return new_tweet.strip()\n",
        "\n",
        "def strip_urls(tweet):\n",
        "  new_tweet = re.sub(r'(https|http)?:\\/\\/(\\w|\\.|\\/|\\?|\\=|\\&|\\%)*\\b', '', tweet, flags=re.MULTILINE)\n",
        "  return new_tweet.strip()\n",
        "\n",
        "def remove_tags(tweet):\n",
        "  return \" \".join([token for token in tweet.split() if not token.startswith(\"@\")])\n",
        "\n",
        "def preprocess_tweet(tweet):\n",
        "  tweet = remove_tags(strip_emoji(strip_urls(tweet)))\n",
        "  tweet = contractions.fix(\" \".join(wordninja.split(tweet)))\n",
        "  tweet = [token.lower() for token in tweet.split() if (len(set(token))>1)]\n",
        "  return \" \".join(tweet)\n",
        "\n",
        "# Preprocessing tweets data\n",
        "print(\"Cleaning and parsing the tweets...\\n\")\n",
        "#train_data_df = train_data_df.iloc[0:10000] #TO-DO: Remove\n",
        "train_data_df.tweet = train_data_df.tweet.apply(preprocess_tweet)\n",
        "print(\"Finished!\\n\")\n",
        "\n",
        "# Preprocessing labels to have classes 0 and 1\n",
        "train_data_df.sentiment = train_data_df.sentiment.apply(lambda value: 1 if value==4 else value)\n",
        "\n",
        "print(\"Sentiment values: \")\n",
        "print(train_data_df.sentiment.value_counts())\n",
        "\n",
        "print(train_data_df.head(5)) \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwxW9HXv-pkL",
        "outputId": "89825130-3bcf-442e-cd5d-dd22661aef42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaning and parsing the tweets...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:2: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
            "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
            "  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze data\n",
        "Estimating the vocabulary size and the vector sequence length to be fed to the model every instance is a crucial step for a good model.\n",
        "\n",
        "This is achieved by analyzing the training dataset, by plotting the \n",
        "distribution of tweet lengths across the training data.\n"
      ],
      "metadata": {
        "id": "jUoDy_SLa3WD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimating vocab size and max sequence length to allow in vectorization layer.\n",
        "def tweet_length(tweet):\n",
        "  return len([token for token in tweet.split()])\n",
        "\n",
        "tweet_lengths = [tweet_length(tweet) for tweet in train_data_df.tweet.tolist()]\n",
        "sns.distplot(tweet_lengths)\n",
        "\n",
        "# Unique words\n",
        "unique_words = set([token for tweet in train_data_df.tweet for token in tweet.split()])\n",
        "print(\"Total Unique Words:\", len(unique_words))\n",
        "\n",
        "# Counting Total Words and Stop Words\n",
        "nltk.download(\"stopwords\")\n",
        "stop_words = stopwords.words(\"english\")\n",
        "total_words = [token for tweet in train_data_df.tweet for token in tweet.split()]\n",
        "total_stop_words = [token for tweet in train_data_df.tweet for token in tweet.split() if token in stop_words]\n",
        "print('Total words', len(total_words))\n",
        "print('Total stop words', len(total_stop_words))\n",
        "print('Ratio of total words to total stop words:', len(total_words)/len(total_stop_words))"
      ],
      "metadata": {
        "id": "LzakgPb2QY-T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building a Data Pipeline"
      ],
      "metadata": {
        "id": "hY1gEQc0idO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Data Pipeline Function using TF Dataset API\n",
        "\"\"\" \n",
        "def data_input_fn(texts, labels, batch_size=32, is_training=True):\n",
        "  # Convert the inputs to a Dataset.\n",
        "  dataset = tf.data.Dataset.from_tensor_slices((texts,labels))\n",
        "  # Shuffle, repeat, and batch the examples.\n",
        "  dataset = dataset.cache()\n",
        "  if is_training:\n",
        "    dataset = dataset.shuffle(1000, reshuffle_each_iteration=True)\n",
        "    dataset = dataset.repeat()\n",
        "  dataset = dataset.batch(batch_size, drop_remainder=True)\n",
        "  # Return the dataset.\n",
        "  return dataset\n",
        "\n",
        "#Split training dataset into train and validation sets\n",
        "train_df, val_df = train_test_split(train_data_df, test_size=0.2)\n",
        "\n",
        "# Data pipelines for 2 different datasets\n",
        "training_dataset = data_input_fn(train_df.tweet, train_df.sentiment, batch_size=1024)\n",
        "validation_dataset = data_input_fn(val_df.tweet, val_df.sentiment, batch_size=128, is_training=False)\n"
      ],
      "metadata": {
        "id": "qL57FQswidqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model\n",
        "### Text Vectorization Layer\n",
        "Before feeding the text to the model, it is common practice to vectorize it first.\n",
        "\n",
        "This can be achieved through the Text Vectorization API from tf.keras."
      ],
      "metadata": {
        "id": "rspb76kIluXY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating Vectorization Layer\n",
        "max_features = 75000\n",
        "max_len = 50\n",
        "\n",
        "vectorization_layer = tf.keras.layers.experimental.preprocessing.TextVectorization(\n",
        "    max_tokens=max_features, output_sequence_length=max_len)\n",
        "vectorization_layer.adapt(train_df.tweet.values)"
      ],
      "metadata": {
        "id": "69WnE54Flure"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Design"
      ],
      "metadata": {
        "id": "u6RObQuln25S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create Model Func\n",
        "def create_model():\n",
        "  words = tf.keras.Input(shape=(1,), dtype=tf.string)\n",
        "  vectors = vectorization_layer(words)\n",
        "  embeddings = tf.keras.layers.Embedding(input_dim=max_features+1, output_dim=128)(vectors)\n",
        "  output = tf.keras.layers.LSTM(256, return_sequences=True, name='LSTM_1')(embeddings)\n",
        "  output = tf.keras.layers.LSTM(256, name='LSTM_2')(output)\n",
        "  output = tf.keras.layers.Dropout(0.3)(output)\n",
        "  output = tf.keras.layers.Dense(64, activation='relu', name='Dense_3')(output)\n",
        "  output = tf.keras.layers.Dense(1,activation='sigmoid', name='Output')(output)\n",
        "\n",
        "  model = tf.keras.models.Model(words,output)\n",
        "  return model"
      ],
      "metadata": {
        "id": "ugoV8xCNn2VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 1024\n",
        "epochs = 3\n",
        "steps_per_epoch = train_df.tweet.shape[0] // batch_size\n",
        "model = create_model()\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Fitting the model\n",
        "model.fit(training_dataset, epochs=epochs, batch_size=batch_size, \n",
        "          steps_per_epoch=steps_per_epoch, validation_data=validation_dataset)"
      ],
      "metadata": {
        "id": "wWnlpiueoata"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Model\n",
        "import os\n",
        "MODEL_DIR = \"/content/mydrive/SentimentAnalysis/Output\"\n",
        "version = 1\n",
        "export_path = os.path.join(MODEL_DIR, str(version))\n",
        "print('export_path = {}\\n'.format(export_path))\n",
        "\n",
        "tf.keras.models.save_model(\n",
        "    model,\n",
        "    export_path,\n",
        "    overwrite=True,\n",
        "    include_optimizer=True,\n",
        "    save_format='h5',\n",
        "    signatures=None,\n",
        "    options=None\n",
        ")\n",
        "\n",
        "# Check the path\n",
        "print('\\nSaved model:')\n",
        "!ls -l {export_path}\n",
        "\n",
        "# Using SavedModelCLI to check if model is persisted properly\n",
        "!saved_model_cli show --dir {export_path} --all"
      ],
      "metadata": {
        "id": "cDGS7CzMocnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading and Evaluation of Model\n",
        "\n",
        "test_data_df = pd.read_csv(os.path.join(input_path, 'testdata.manual.2009.06.14.csv'),  encoding='latin-1', usecols=[0,5], names=['sentiment','tweet'])\n",
        "print(test_data_df.sentiment.value_counts()/test_data_df.shape[0])\n",
        "\n",
        "# Preprocessing tweets data\n",
        "print(\"Cleaning and parsing the tweets...\\n\")\n",
        "test_data_df = test_data_df.iloc[0:1000] #TO-DO: Remove\n",
        "test_data_df.tweet = test_data_df.tweet.apply(preprocess_tweet) #TO-DO\n",
        "print(\"Finished!\\n\")\n",
        "\n",
        "test_data_df = test_data_df[test_data_df.sentiment!=2] #Remove intermediate polarities\n",
        "test_data_df.sentiment.value_counts()/test_data_df.shape[0]\n",
        "\n",
        "test_data_df.sentiment = test_data_df.sentiment.apply(lambda value: 1 if value==4 else value)\n",
        "\n",
        "print(test_data_df.sentiment.value_counts()/test_data_df.shape[0])\n",
        "\n",
        "\n",
        "# Preprocessing labels to have classes 0 and 1\n",
        "test_data_df.sentiment = test_data_df.sentiment.apply(lambda value: 1 if value==4 else value)\n",
        "\n",
        "#Create data pipeline for test\n",
        "test_dataset = data_input_fn(test_data_df.tweet, test_data_df.sentiment, batch_size=128, is_training=False)\n",
        "\n",
        "\n",
        "model = tf.keras.models.load_model(export_path)\n",
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "jKgxojXgpahr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}